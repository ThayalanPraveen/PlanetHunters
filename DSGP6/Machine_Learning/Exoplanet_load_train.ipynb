{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Import packages\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sklearn Packages\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Sklearn Evaluation Metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error, precision_score, confusion_matrix, accuracy_score\n",
        "\n",
        "# Visualizes all the columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#array = joblib.load('Dataset.joblib')\n",
        "\n",
        "# if it does't work use the line below\n",
        "# array = joblib.load('Dataset1.joblib')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#len(array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df = pd.DataFrame(array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df.to_csv(\"Exoplanet_Flux_Dataset.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "#max_length = df.shape[1] - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "#max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df['EC'] = df[1].apply(lambda x: 1 if x == 'CONFIRMED' else 1 if x == 'CANDIDATE' else 0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "#features = df.drop(columns=[0,1,'EC'])\n",
        "#target = df.EC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'features.joblib'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/thayalanpraveen/Documents/GitHub/PlanetHunters/DSGP6/Machine_Learning/Exoplanet_load_train.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/thayalanpraveen/Documents/GitHub/PlanetHunters/DSGP6/Machine_Learning/Exoplanet_load_train.ipynb#ch0000025?line=0'>1</a>\u001b[0m features \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mfeatures.joblib\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thayalanpraveen/Documents/GitHub/PlanetHunters/DSGP6/Machine_Learning/Exoplanet_load_train.ipynb#ch0000025?line=1'>2</a>\u001b[0m target \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mtarget.joblib\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/Project_Interpreter/Env/lib/python3.9/site-packages/joblib/numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/thayalanpraveen/Documents/Project_Interpreter/Env/lib/python3.9/site-packages/joblib/numpy_pickle.py?line=576'>577</a>\u001b[0m         obj \u001b[39m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    <a href='file:///Users/thayalanpraveen/Documents/Project_Interpreter/Env/lib/python3.9/site-packages/joblib/numpy_pickle.py?line=577'>578</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/thayalanpraveen/Documents/Project_Interpreter/Env/lib/python3.9/site-packages/joblib/numpy_pickle.py?line=578'>579</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    <a href='file:///Users/thayalanpraveen/Documents/Project_Interpreter/Env/lib/python3.9/site-packages/joblib/numpy_pickle.py?line=579'>580</a>\u001b[0m         \u001b[39mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[39mas\u001b[39;00m fobj:\n\u001b[1;32m    <a href='file:///Users/thayalanpraveen/Documents/Project_Interpreter/Env/lib/python3.9/site-packages/joblib/numpy_pickle.py?line=580'>581</a>\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/thayalanpraveen/Documents/Project_Interpreter/Env/lib/python3.9/site-packages/joblib/numpy_pickle.py?line=581'>582</a>\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/thayalanpraveen/Documents/Project_Interpreter/Env/lib/python3.9/site-packages/joblib/numpy_pickle.py?line=582'>583</a>\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/thayalanpraveen/Documents/Project_Interpreter/Env/lib/python3.9/site-packages/joblib/numpy_pickle.py?line=583'>584</a>\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'features.joblib'"
          ]
        }
      ],
      "source": [
        "features = joblib.load(\"features.joblib\")\n",
        "target = joblib.load(\"target.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#medians=features.median(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df_dict = features.to_dict(orient='index')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"for x in range(0,len(df_dict)):\n",
        "    for y in range(2,max_length+1):\n",
        "        df_dict[x][y] = df_dict[x][y] /medians[x] -1\"\"\"\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"for x in range(0,len(df_dict)):\n",
        "    for y in range(2,max_length+1):\n",
        "        if str(df_dict[x][y]) == \"nan\":\n",
        "            df_dict[x][y] = -999\n",
        "        else:\n",
        "            df_dict[x][y] = float(df_dict[x][y])\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#features = pd.DataFrame.from_dict(df_dict,orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=1, test_size=.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "\n",
        "def evaluation(y_true, y_pred):\n",
        "    \n",
        "# Print Accuracy, Recall, F1 Score, and Precision metrics.\n",
        "    print('Evaluation Metrics:')\n",
        "    print('Accuracy: ' + str(metrics.accuracy_score(y_test, y_pred)))\n",
        "    print('Recall: ' + str(metrics.recall_score(y_test, y_pred)))\n",
        "    print('F1 Score: ' + str(metrics.f1_score(y_test, y_pred)))\n",
        "    print('Precision: ' + str(metrics.precision_score(y_test, y_pred)))\n",
        "    \n",
        "# Print Confusion Matrix\n",
        "    print('\\nConfusion Matrix:')\n",
        "    print(' TN,  FP, FN, TP')\n",
        "    print(confusion_matrix(y_true, y_pred).ravel())\n",
        "    \n",
        "# Function Prints best parameters for GridSearchCV\n",
        "def print_results(results):\n",
        "    print('Best Parameters: {}\\n'.format(results.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tree = DecisionTreeClassifier()\n",
        "\n",
        "# Fitting Model to the train set\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Evaluating model\n",
        "evaluation(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "parameter_n_estimators = [500]\n",
        "for i in tqdm(parameter_n_estimators):\n",
        "    # Instantiate model\n",
        "    forest = RandomForestClassifier(n_estimators=i, criterion='gini')\n",
        "    # Fitting Model to the train set\n",
        "    forest.fit(X_train, y_train)\n",
        "    # Predicting on the test set\n",
        "    y_pred = forest.predict(X_test)\n",
        "\n",
        "    # Evaluating model\n",
        "    evaluation(y_test, y_pred)\n",
        "    print('Tree: %s ' % (i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradient_booster = GradientBoostingClassifier(learning_rate=0.1)\n",
        "gradient_booster.get_params()\n",
        "\n",
        "gradient_booster.fit(X_train,y_train)\n",
        "y_pred = gradient_booster.predict(X_test)\n",
        "evaluation(y_test, y_pred)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lightkurve as lk\n",
        "\n",
        "T_name = 'TIC 145241359'\n",
        "search_result = lk.search_lightcurve(T_name)\n",
        "data = []\n",
        "for x in range(0,17):\n",
        "    try:\n",
        "        lc = search_result[x].download() \n",
        "        y = lc.flux\n",
        "        for i in range(1,len(y),10):\n",
        "            try:\n",
        "                data.append(float(y[i].value))\n",
        "            except:\n",
        "                pass\n",
        "    except :\n",
        "        pass\n",
        "\n",
        "arr2 = pd.DataFrame(data)\n",
        "medians= arr2.median(axis=0)\n",
        "arr3 =[]\n",
        "for x in range(0,53255-1):\n",
        "    try:\n",
        "        arr3.append((arr2[0][x] / medians)-1)\n",
        "    except:\n",
        "        arr3.append(-999)\n",
        "\n",
        "for x in range(0,53255-1):\n",
        "    temp = str(arr3[x])\n",
        "    if (temp == 'nan') or (temp == '0   NaN\\ndtype: float64') :\n",
        "        arr3[x] = -999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = forest.predict([arr3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(forest,\"random_forest_model.joblib\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Exoplanet_ML.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "c389ce371d970bfbc3db3abccddfe5bee565366d23406449d68d8a0e44b07ec2"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('Env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

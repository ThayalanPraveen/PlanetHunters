{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Import packages\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sklearn Packages\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Sklearn Evaluation Metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error, precision_score, confusion_matrix, accuracy_score\n",
        "\n",
        "# Visualizes all the columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "916"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "array = joblib.load('Dataset.joblib')\n",
        "len(array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = df.shape[1] - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['EC'] = df[1].apply(lambda x: 1 if x == 'CONFIRMED' else 1 if x == 'CANDIDATE' else 0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = df.drop(columns=[0,1,'EC'])\n",
        "target = df.EC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "medians=features.median(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_dict = features.to_dict(orient='index')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "for x in range(0,len(df_dict)):\n",
        "    for y in range(2,max_length+2):\n",
        "        df_dict[x][y] = df_dict[x][y] /medians[x] -1\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for x in range(0,len(df_dict)):\n",
        "    for y in range(2,max_length+2):\n",
        "        if str(df_dict[x][y]) == \"nan\":\n",
        "            df_dict[x][y] = -999\n",
        "        else:\n",
        "            df_dict[x][y] = float(df_dict[x][y])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = pd.DataFrame.from_dict(df_dict,orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=1, test_size=.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "\n",
        "def evaluation(y_true, y_pred):\n",
        "    \n",
        "# Print Accuracy, Recall, F1 Score, and Precision metrics.\n",
        "    print('Evaluation Metrics:')\n",
        "    print('Accuracy: ' + str(metrics.accuracy_score(y_test, y_pred)))\n",
        "    print('Recall: ' + str(metrics.recall_score(y_test, y_pred)))\n",
        "    print('F1 Score: ' + str(metrics.f1_score(y_test, y_pred)))\n",
        "    print('Precision: ' + str(metrics.precision_score(y_test, y_pred)))\n",
        "    \n",
        "# Print Confusion Matrix\n",
        "    print('\\nConfusion Matrix:')\n",
        "    print(' TN,  FP, FN, TP')\n",
        "    print(confusion_matrix(y_true, y_pred).ravel())\n",
        "    \n",
        "# Function Prints best parameters for GridSearchCV\n",
        "def print_results(results):\n",
        "    print('Best Parameters: {}\\n'.format(results.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metrics:\n",
            "Accuracy: 0.9344978165938864\n",
            "Recall: 0.9385474860335196\n",
            "F1 Score: 0.9572649572649573\n",
            "Precision: 0.9767441860465116\n",
            "\n",
            "Confusion Matrix:\n",
            " TN,  FP, FN, TP\n",
            "[ 46   4  11 168]\n"
          ]
        }
      ],
      "source": [
        "tree = DecisionTreeClassifier()\n",
        "\n",
        "# Fitting Model to the train set\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Evaluating model\n",
        "evaluation(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28fc13adb7b34200aaf8923806d1a2d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metrics:\n",
            "Accuracy: 0.9344978165938864\n",
            "Recall: 0.9441340782122905\n",
            "F1 Score: 0.9575070821529744\n",
            "Precision: 0.9712643678160919\n",
            "\n",
            "Confusion Matrix:\n",
            " TN,  FP, FN, TP\n",
            "[ 45   5  10 169]\n",
            "Tree: 500 \n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "parameter_n_estimators = [500]\n",
        "for i in tqdm(parameter_n_estimators):\n",
        "    # Instantiate model\n",
        "    forest = RandomForestClassifier(n_estimators=i, criterion='gini')\n",
        "    # Fitting Model to the train set\n",
        "    forest.fit(X_train, y_train)\n",
        "    # Predicting on the test set\n",
        "    y_pred = forest.predict(X_test)\n",
        "\n",
        "    # Evaluating model\n",
        "    evaluation(y_test, y_pred)\n",
        "    print('Tree: %s ' % (i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metrics:\n",
            "Accuracy: 0.834061135371179\n",
            "Recall: 0.8938547486033519\n",
            "F1 Score: 0.8938547486033519\n",
            "Precision: 0.8938547486033519\n",
            "\n",
            "Confusion Matrix:\n",
            " TN,  FP, FN, TP\n",
            "[ 31  19  19 160]\n"
          ]
        }
      ],
      "source": [
        "knn = KNeighborsClassifier(leaf_size=8, metric='manhattan',weights='uniform')\n",
        "\n",
        "# Fitting Model to the train set\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluating model\n",
        "evaluation(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    1.000336\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import lightkurve as lk\n",
        "\n",
        "T_name = 'TIC 145241359'\n",
        "search_result = lk.search_lightcurve(T_name)\n",
        "data = []\n",
        "for x in range(0,17):\n",
        "    try:\n",
        "        lc = search_result[x].download() \n",
        "        y = lc.flux\n",
        "        for i in range(1,len(y),10):\n",
        "            try:\n",
        "                data.append(float(y[i].value))\n",
        "            except:\n",
        "                pass\n",
        "    except :\n",
        "        pass\n",
        "\n",
        "arr2 = pd.DataFrame(data)\n",
        "medians= arr2.median(axis=0)\n",
        "arr3 =[]\n",
        "for x in range(0,max_length):\n",
        "    try:\n",
        "        arr3.append((arr2[0][x] / medians)-1)\n",
        "    except:\n",
        "        arr3.append(-999)\n",
        "\n",
        "for x in range(0,max_length):\n",
        "    temp = str(arr3[x])\n",
        "    if temp == 'nan' :\n",
        "        arr3[x] = -999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "test1 = forest.predict([arr3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1]\n"
          ]
        }
      ],
      "source": [
        "print(test1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Exoplanet_ML.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

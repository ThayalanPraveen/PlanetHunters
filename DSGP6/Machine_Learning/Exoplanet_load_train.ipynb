{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Import packages\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sklearn Packages\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Sklearn Evaluation Metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error, precision_score, confusion_matrix, accuracy_score\n",
        "\n",
        "# Visualizes all the columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "array = joblib.load('Dataset.joblib')\n",
        "\n",
        "# if it does't work use the line below\n",
        "# array = joblib.load('Dataset1.joblib')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3768"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df.to_csv(\"Exoplanet_Flux_Dataset.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3768, 53256)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = df.shape[1] - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "53255"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['EC'] = df[1].apply(lambda x: 1 if x == 'CONFIRMED' else 1 if x == 'CANDIDATE' else 0 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = df.drop(columns=[0,1,'EC'])\n",
        "target = df.EC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "medians=features.median(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_dict = features.to_dict(orient='index')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "for x in range(0,len(df_dict)):\n",
        "    for y in range(2,max_length+1):\n",
        "        df_dict[x][y] = df_dict[x][y] /medians[x] -1\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for x in range(0,len(df_dict)):\n",
        "    for y in range(2,max_length+1):\n",
        "        if str(df_dict[x][y]) == \"nan\":\n",
        "            df_dict[x][y] = -999\n",
        "        else:\n",
        "            df_dict[x][y] = float(df_dict[x][y])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = pd.DataFrame.from_dict(df_dict,orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=1, test_size=.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "\n",
        "def evaluation(y_true, y_pred):\n",
        "    \n",
        "# Print Accuracy, Recall, F1 Score, and Precision metrics.\n",
        "    print('Evaluation Metrics:')\n",
        "    print('Accuracy: ' + str(metrics.accuracy_score(y_test, y_pred)))\n",
        "    print('Recall: ' + str(metrics.recall_score(y_test, y_pred)))\n",
        "    print('F1 Score: ' + str(metrics.f1_score(y_test, y_pred)))\n",
        "    print('Precision: ' + str(metrics.precision_score(y_test, y_pred)))\n",
        "    \n",
        "# Print Confusion Matrix\n",
        "    print('\\nConfusion Matrix:')\n",
        "    print(' TN,  FP, FN, TP')\n",
        "    print(confusion_matrix(y_true, y_pred).ravel())\n",
        "    \n",
        "# Function Prints best parameters for GridSearchCV\n",
        "def print_results(results):\n",
        "    print('Best Parameters: {}\\n'.format(results.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metrics:\n",
            "Accuracy: 0.7016985138004246\n",
            "Recall: 0.7988422575976846\n",
            "F1 Score: 0.7971119133574007\n",
            "Precision: 0.7953890489913544\n",
            "\n",
            "Confusion Matrix:\n",
            " TN,  FP, FN, TP\n",
            "[109 142 139 552]\n"
          ]
        }
      ],
      "source": [
        "tree = DecisionTreeClassifier()\n",
        "\n",
        "# Fitting Model to the train set\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "# Evaluating model\n",
        "evaluation(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c49f7222237452d8ac93511323e4a56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metrics:\n",
            "Accuracy: 0.7579617834394905\n",
            "Recall: 0.9479015918958031\n",
            "F1 Score: 0.8517555266579974\n",
            "Precision: 0.7733175914994097\n",
            "\n",
            "Confusion Matrix:\n",
            " TN,  FP, FN, TP\n",
            "[ 59 192  36 655]\n",
            "Tree: 500 \n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "parameter_n_estimators = [500]\n",
        "for i in tqdm(parameter_n_estimators):\n",
        "    # Instantiate model\n",
        "    forest = RandomForestClassifier(n_estimators=i, criterion='gini')\n",
        "    # Fitting Model to the train set\n",
        "    forest.fit(X_train, y_train)\n",
        "    # Predicting on the test set\n",
        "    y_pred = forest.predict(X_test)\n",
        "\n",
        "    # Evaluating model\n",
        "    evaluation(y_test, y_pred)\n",
        "    print('Tree: %s ' % (i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metrics:\n",
            "Accuracy: 0.7574468085106383\n",
            "Recall: 0.9572039942938659\n",
            "F1 Score: 0.8547770700636943\n",
            "Precision: 0.7721518987341772\n",
            "\n",
            "Confusion Matrix:\n",
            " TN,  FP, FN, TP\n",
            "[ 41 198  30 671]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gradient_booster = GradientBoostingClassifier(learning_rate=0.1)\n",
        "gradient_booster.get_params()\n",
        "\n",
        "gradient_booster.fit(X_train,y_train)\n",
        "y_pred = gradient_booster.predict(X_test)\n",
        "evaluation(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lightkurve as lk\n",
        "\n",
        "T_name = 'TIC 145241359'\n",
        "search_result = lk.search_lightcurve(T_name)\n",
        "data = []\n",
        "for x in range(0,17):\n",
        "    try:\n",
        "        lc = search_result[x].download() \n",
        "        y = lc.flux\n",
        "        for i in range(1,len(y),10):\n",
        "            try:\n",
        "                data.append(float(y[i].value))\n",
        "            except:\n",
        "                pass\n",
        "    except :\n",
        "        pass\n",
        "\n",
        "arr2 = pd.DataFrame(data)\n",
        "medians= arr2.median(axis=0)\n",
        "arr3 =[]\n",
        "for x in range(0,max_length-1):\n",
        "    try:\n",
        "        arr3.append((arr2[0][x] / medians)-1)\n",
        "    except:\n",
        "        arr3.append(-999)\n",
        "\n",
        "for x in range(0,max_length-1):\n",
        "    temp = str(arr3[x])\n",
        "    if (temp == 'nan') or (temp == '0   NaN\\ndtype: float64') :\n",
        "        arr3[x] = -999"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = forest.predict([arr3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0]\n"
          ]
        }
      ],
      "source": [
        "print(test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Exoplanet_ML.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "c389ce371d970bfbc3db3abccddfe5bee565366d23406449d68d8a0e44b07ec2"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('Env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

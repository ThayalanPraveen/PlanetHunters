{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a22e7-bcd4-41c1-ae92-df1b9b5d100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading csv\n",
      "1-started\n",
      "2-started\n",
      "3-started\n",
      "4-started\n",
      "3-loaded\n",
      "3-Creating dataset\n",
      "1-loaded\n",
      "1-Creating dataset\n",
      "4-loaded\n",
      "4-Creating dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: Mean of empty slice\n",
      "<string>:6: RuntimeWarning: Degrees of freedom <= 0 for slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Download/Stitch error! #\n",
      "2-loaded\n",
      "2-Creating dataset\n",
      "8601\n",
      "8602\n",
      "7301\n",
      "7302\n",
      "8603\n",
      "7303\n",
      "8604\n",
      "7304\n",
      "8605\n",
      "8606\n",
      "9202\n",
      "8607\n",
      "9203\n",
      "8608\n",
      "8609\n",
      "8610\n",
      "1-Download/Stitch error! #\n",
      "8611\n",
      "6202\n",
      "8612\n",
      "2-Download/Stitch error! #\n",
      "7306\n",
      "8613\n",
      "7307\n",
      "8614\n",
      "7308\n",
      "8615\n",
      "7309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: Mean of empty slice\n",
      "<string>:6: RuntimeWarning: Degrees of freedom <= 0 for slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Download/Stitch error! #\n",
      "7310\n",
      "8617\n",
      "4-Download/Stitch error! #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: Mean of empty slice\n",
      "<string>:6: RuntimeWarning: Degrees of freedom <= 0 for slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Download/Stitch error! #\n",
      "7311\n",
      "8618\n",
      "7312\n",
      "8619\n",
      "7313\n",
      "8620\n",
      "7314\n",
      "8621\n",
      "7315\n",
      "1-Download/Stitch error! #\n",
      "8622\n",
      "7316\n",
      "8623\n",
      "7317\n",
      "8624\n",
      "4-Download/Stitch error! #\n",
      "8625\n",
      "8626\n",
      "8627\n",
      "8628\n",
      "8629\n",
      "6204\n",
      "8630\n",
      "6205\n",
      "8631\n",
      "6206\n",
      "8632\n",
      "7318\n",
      "6207\n",
      "9207\n",
      "8633\n",
      "8634\n",
      "7319\n",
      "8635\n",
      "7320\n",
      "8636\n",
      "7321\n",
      "8637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/astropy/units/quantity.py:486: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/astropy/units/quantity.py:486: RuntimeWarning: invalid value encountered in multiply\n",
      "  result = super().__array_ufunc__(function, method, *arrays, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: Mean of empty slice\n",
      "<string>:6: RuntimeWarning: Degrees of freedom <= 0 for slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Download/Stitch error! #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: Mean of empty slice\n",
      "<string>:6: RuntimeWarning: Degrees of freedom <= 0 for slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-Download/Stitch error! #\n",
      "7322\n",
      "9208\n",
      "7323\n",
      "8640\n",
      "7324\n",
      "8641\n",
      "8642\n",
      "7325\n",
      "6209\n",
      "8643\n",
      "7326\n",
      "8644\n",
      "6210\n",
      "4-Download/Stitch error! #\n",
      "7327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: Mean of empty slice\n",
      "<string>:6: RuntimeWarning: Degrees of freedom <= 0 for slice.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-Download/Stitch error! #\n",
      "7328\n",
      "7329\n"
     ]
    }
   ],
   "source": [
    "import lightkurve as lk\n",
    "import numpy as np\n",
    "from csv import reader\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "import multiprocessing \n",
    "\n",
    "data_id = []\n",
    "data_class = []\n",
    "data_period = []\n",
    "data_epoch = []\n",
    "data_duration = []\n",
    "\n",
    "print(\"Reading csv\")\n",
    "\n",
    "with open('Dataset_cleaned.csv') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        data_id.append(row[0])\n",
    "        data_class.append(row[1])\n",
    "        data_period.append(float(row[2]))\n",
    "        data_epoch.append(float(row[3]))\n",
    "        data_duration.append(float(row[4]))\n",
    "\n",
    "\n",
    "def one():\n",
    "\n",
    "    print(\"1-started\")\n",
    "\n",
    "    try:\n",
    "        array = joblib.load('Dataset_test_bls_alt_4.joblib')\n",
    "        joblib.dump(array, 'Dataset1_test_bls.joblib')\n",
    "\n",
    "    except:\n",
    "        array = joblib.load('Dataset1_test_bls_alt_4.joblib')\n",
    "        print(\"Dataset.joblib corrupted,using Dataset1.joblib\")\n",
    "        joblib.dump(array, 'Dataset_test_bls_alt_4.joblib')\n",
    "\n",
    "    print(\"1-loaded\")\n",
    "\n",
    "\n",
    "    index = 6000 + len(array[0])\n",
    "\n",
    "    Dataset_global = array[0]\n",
    "    Dataset_local = array[1]\n",
    "\n",
    "    dump_i = 0\n",
    "\n",
    "    \n",
    "    print(\"1-Creating dataset\")\n",
    "\n",
    "    for x in range(index,len(data_id)):\n",
    "\n",
    "        data_global = [data_id[x],data_class[x]]\n",
    "        data_local = [data_id[x],data_class[x]]\n",
    "        try:\n",
    "            lcs = lk.search_lightcurve(data_id[x],author='Kepler',cadence='long').download_all()\n",
    "            lc = lcs.stitch()\n",
    "            lc_clean = lc.remove_outliers(sigma=20, sigma_upper=4)\n",
    "            temp_fold = lc_clean.fold(data_period[x], epoch_time=data_epoch[x])\n",
    "            fractional_duration = (data_duration[x] / 24.0) / data_period[x]\n",
    "            phase_mask = np.abs(temp_fold.phase.value) < (fractional_duration * 1.5)\n",
    "            transit_mask = np.in1d(lc_clean.time.value, temp_fold.time_original.value[phase_mask])\n",
    "            lc_flat, trend_lc = lc_clean.flatten(return_trend=True, mask=transit_mask)\n",
    "            lc_fold = lc_flat.fold(data_period[x], epoch_time=data_epoch[x])\n",
    "            lc_global = lc_fold.bin(time_bin_size=0.005).normalize() - 1\n",
    "            lc_global = (lc_global / np.abs(lc_global.flux.min()) ) * 2.0 + 1\n",
    "\n",
    "            phase_mask = (lc_fold.phase > -4*fractional_duration) & (lc_fold.phase < 4.0*fractional_duration)\n",
    "            lc_zoom = lc_fold[phase_mask]\n",
    "            lc_local = lc_zoom.bin(time_bin_size=0.0005).normalize() - 1\n",
    "            lc_local = (lc_local / np.abs(np.nanmin(lc_local.flux)) ) * 2.0 + 1\n",
    "\n",
    "\n",
    "            for x in range(0,len(lc_global.flux)):\n",
    "                try:\n",
    "                    data_global.append(float(lc_global.flux[x].value))\n",
    "                    data_local.append(float(lc_local.flux[x].value))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            Dataset_global.append(data_global)\n",
    "            Dataset_local.append(data_local)\n",
    "            index = index + 1\n",
    "            print(index)\n",
    "            dump_i = dump_i + 1\n",
    "            if index == 6998 :\n",
    "                print(\"1-Dumping\")\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset_test_bls_alt_4.joblib')\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset1_test_bls_alt_4.joblib')\n",
    "                print(\"1-Dump Complete\")\n",
    "                break\n",
    "            if dump_i == 100 :\n",
    "                dump_i = 0\n",
    "                print(\"1-Dumping\")\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset_test_bls_alt_4.joblib')\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset1_test_bls_alt_4.joblib')\n",
    "                print(\"1-Dump Complete\")\n",
    "\n",
    "        except :\n",
    "            print(\"1-Download/Stitch error! #\")\n",
    "            index = index + 1 \n",
    "            \n",
    "def two():\n",
    "    \n",
    "    print(\"2-started\")\n",
    "\n",
    "    try:\n",
    "        array = joblib.load('Dataset_test_bls_alt_5.joblib')\n",
    "        joblib.dump(array, 'Dataset1_test_bls_alt_5.joblib')\n",
    "\n",
    "    except:\n",
    "        array = joblib.load('Dataset1_test_bls_alt_5.joblib')\n",
    "        print(\"Dataset.joblib corrupted,using Dataset1.joblib\")\n",
    "        joblib.dump(array, 'Dataset_test_bls_alt_5.joblib')\n",
    "\n",
    "    print(\"2-loaded\")\n",
    "\n",
    "\n",
    "    index = 7000 + len(array[0])\n",
    "\n",
    "    Dataset_global = array[0]\n",
    "    Dataset_local = array[1]\n",
    "\n",
    "    dump_i = 0\n",
    "\n",
    "    print(\"2-Creating dataset\")\n",
    "\n",
    "    for x in range(index,len(data_id)):\n",
    "\n",
    "        data_global = [data_id[x],data_class[x]]\n",
    "        data_local = [data_id[x],data_class[x]]\n",
    "        try:\n",
    "            lcs = lk.search_lightcurve(data_id[x],author='Kepler',cadence='long').download_all()\n",
    "            lc = lcs.stitch()\n",
    "            lc_clean = lc.remove_outliers(sigma=20, sigma_upper=4)\n",
    "            temp_fold = lc_clean.fold(data_period[x], epoch_time=data_epoch[x])\n",
    "            fractional_duration = (data_duration[x] / 24.0) / data_period[x]\n",
    "            phase_mask = np.abs(temp_fold.phase.value) < (fractional_duration * 1.5)\n",
    "            transit_mask = np.in1d(lc_clean.time.value, temp_fold.time_original.value[phase_mask])\n",
    "            lc_flat, trend_lc = lc_clean.flatten(return_trend=True, mask=transit_mask)\n",
    "            lc_fold = lc_flat.fold(data_period[x], epoch_time=data_epoch[x])\n",
    "            lc_global = lc_fold.bin(time_bin_size=0.005).normalize() - 1\n",
    "            lc_global = (lc_global / np.abs(lc_global.flux.min()) ) * 2.0 + 1\n",
    "\n",
    "            phase_mask = (lc_fold.phase > -4*fractional_duration) & (lc_fold.phase < 4.0*fractional_duration)\n",
    "            lc_zoom = lc_fold[phase_mask]\n",
    "            lc_local = lc_zoom.bin(time_bin_size=0.0005).normalize() - 1\n",
    "            lc_local = (lc_local / np.abs(np.nanmin(lc_local.flux)) ) * 2.0 + 1\n",
    "\n",
    "\n",
    "            for x in range(0,len(lc_global.flux)):\n",
    "                try:\n",
    "                    data_global.append(float(lc_global.flux[x].value))\n",
    "                    data_local.append(float(lc_local.flux[x].value))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            Dataset_global.append(data_global)\n",
    "            Dataset_local.append(data_local)\n",
    "            index = index + 1\n",
    "            print(index)\n",
    "            dump_i = dump_i + 1\n",
    "            if index == 7998 :\n",
    "                print(\"2-Dumping\")\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset_test_bls_alt_5.joblib')\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset1_test_bls_alt_5.joblib')\n",
    "                print(\"2-Dump Complete\")\n",
    "                break\n",
    "            if dump_i == 100 :\n",
    "                dump_i = 0\n",
    "                print(\"2-Dumping\")\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset_test_bls_alt_5.joblib')\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset1_test_bls_alt_5.joblib')\n",
    "                print(\"2-Dump Complete\")\n",
    "\n",
    "        except :\n",
    "            print(\"2-Download/Stitch error! #\")\n",
    "            index = index + 1 \n",
    "\n",
    "def three():\n",
    "    \n",
    "    print(\"3-started\")\n",
    "\n",
    "    try:\n",
    "        array = joblib.load('Dataset_test_bls_alt_6.joblib')\n",
    "        joblib.dump(array, 'Dataset1_test_bls_alt_6.joblib')\n",
    "\n",
    "    except:\n",
    "        array = joblib.load('Dataset1_test_bls_alt_6.joblib')\n",
    "        print(\"Dataset.joblib corrupted,using Dataset1.joblib\")\n",
    "        joblib.dump(array, 'Dataset_test_bls_alt_6.joblib')\n",
    "\n",
    "    print(\"3-loaded\")\n",
    "\n",
    "\n",
    "    index = 8000 + len(array[0])\n",
    "\n",
    "    Dataset_global = array[0]\n",
    "    Dataset_local = array[1]\n",
    "\n",
    "    dump_i = 0\n",
    "\n",
    "\n",
    "    print(\"3-Creating dataset\")\n",
    "\n",
    "    for x in range(index,len(data_id)):\n",
    "\n",
    "        data_global = [data_id[x],data_class[x]]\n",
    "        data_local = [data_id[x],data_class[x]]\n",
    "        try:\n",
    "            lcs = lk.search_lightcurve(data_id[x],author='Kepler',cadence='long').download_all()\n",
    "            lc = lcs.stitch()\n",
    "            lc_clean = lc.remove_outliers(sigma=20, sigma_upper=4)\n",
    "            temp_fold = lc_clean.fold(data_period[x], epoch_time=data_epoch[x])\n",
    "            fractional_duration = (data_duration[x] / 24.0) / data_period[x]\n",
    "            phase_mask = np.abs(temp_fold.phase.value) < (fractional_duration * 1.5)\n",
    "            transit_mask = np.in1d(lc_clean.time.value, temp_fold.time_original.value[phase_mask])\n",
    "            lc_flat, trend_lc = lc_clean.flatten(return_trend=True, mask=transit_mask)\n",
    "            lc_fold = lc_flat.fold(data_period[x], epoch_time=data_epoch[x])\n",
    "            lc_global = lc_fold.bin(time_bin_size=0.005).normalize() - 1\n",
    "            lc_global = (lc_global / np.abs(lc_global.flux.min()) ) * 2.0 + 1\n",
    "\n",
    "            phase_mask = (lc_fold.phase > -4*fractional_duration) & (lc_fold.phase < 4.0*fractional_duration)\n",
    "            lc_zoom = lc_fold[phase_mask]\n",
    "            lc_local = lc_zoom.bin(time_bin_size=0.0005).normalize() - 1\n",
    "            lc_local = (lc_local / np.abs(np.nanmin(lc_local.flux)) ) * 2.0 + 1\n",
    "\n",
    "\n",
    "            for x in range(0,len(lc_global.flux)):\n",
    "                try:\n",
    "                    data_global.append(float(lc_global.flux[x].value))\n",
    "                    data_local.append(float(lc_local.flux[x].value))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            Dataset_global.append(data_global)\n",
    "            Dataset_local.append(data_local)\n",
    "            index = index + 1\n",
    "            print(index)\n",
    "            dump_i = dump_i + 1\n",
    "            if index == 8998 :\n",
    "                print(\"3-Dumping\")\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset_test_bls_alt_6.joblib')\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset1_test_bls_alt_6.joblib')\n",
    "                print(\"3-Dump Complete\")\n",
    "                break\n",
    "            if dump_i == 100 :\n",
    "                dump_i = 0\n",
    "                print(\"3-Dumping\")\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset_test_bls_alt_6.joblib')\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset1_test_bls_alt_6.joblib')\n",
    "                print(\"3-Dump Complete\")\n",
    "\n",
    "        except :\n",
    "            print(\"3-Download/Stitch error! #\")\n",
    "            index = index + 1 \n",
    "\n",
    "\n",
    "def four():\n",
    "\n",
    "    print(\"4-started\")\n",
    "\n",
    "    try:\n",
    "        array = joblib.load('Dataset_test_bls_alt_7.joblib')\n",
    "        joblib.dump(array, 'Dataset1_test_bls_alt_7.joblib')\n",
    "\n",
    "    except:\n",
    "        array = joblib.load('Dataset1_test_bls_alt_7.joblib')\n",
    "        print(\"Dataset.joblib corrupted,using Dataset1.joblib\")\n",
    "        joblib.dump(array, 'Dataset_test_bls_alt_7.joblib')\n",
    "\n",
    "    print(\"4-loaded\")\n",
    "\n",
    "\n",
    "    index = 9000 + len(array[0])\n",
    "\n",
    "    Dataset_global = array[0]\n",
    "    Dataset_local = array[1]\n",
    "\n",
    "    dump_i = 0\n",
    "\n",
    "\n",
    "    print(\"4-Creating dataset\")\n",
    "\n",
    "    for x in range(index,len(data_id)):\n",
    "\n",
    "        data_global = [data_id[x],data_class[x]]\n",
    "        data_local = [data_id[x],data_class[x]]\n",
    "        try:\n",
    "            lcs = lk.search_lightcurve(data_id[x],author='Kepler',cadence='long').download_all()\n",
    "            lc = lcs.stitch()\n",
    "            lc_clean = lc.remove_outliers(sigma=20, sigma_upper=4)\n",
    "            temp_fold = lc_clean.fold(data_period[x], epoch_time=data_epoch[x])\n",
    "            fractional_duration = (data_duration[x] / 24.0) / data_period[x]\n",
    "            phase_mask = np.abs(temp_fold.phase.value) < (fractional_duration * 1.5)\n",
    "            transit_mask = np.in1d(lc_clean.time.value, temp_fold.time_original.value[phase_mask])\n",
    "            lc_flat, trend_lc = lc_clean.flatten(return_trend=True, mask=transit_mask)\n",
    "            lc_fold = lc_flat.fold(data_period[x], epoch_time=data_epoch[x])\n",
    "            lc_global = lc_fold.bin(time_bin_size=0.005).normalize() - 1\n",
    "            lc_global = (lc_global / np.abs(lc_global.flux.min()) ) * 2.0 + 1\n",
    "\n",
    "            phase_mask = (lc_fold.phase > -4*fractional_duration) & (lc_fold.phase < 4.0*fractional_duration)\n",
    "            lc_zoom = lc_fold[phase_mask]\n",
    "            lc_local = lc_zoom.bin(time_bin_size=0.0005).normalize() - 1\n",
    "            lc_local = (lc_local / np.abs(np.nanmin(lc_local.flux)) ) * 2.0 + 1\n",
    "\n",
    "\n",
    "            for x in range(0,len(lc_global.flux)):\n",
    "                try:\n",
    "                    data_global.append(float(lc_global.flux[x].value))\n",
    "                    data_local.append(float(lc_local.flux[x].value))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            Dataset_global.append(data_global)\n",
    "            Dataset_local.append(data_local)\n",
    "            index = index + 1\n",
    "            print(index)\n",
    "            dump_i = dump_i + 1\n",
    "            if index == 9998 :\n",
    "                print(\"4-Dumping\")\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset_test_bls_alt_7.joblib')\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset1_test_bls_alt_7.joblib')\n",
    "                print(\"4-Dump Complete\")\n",
    "                break\n",
    "            if dump_i == 100 :\n",
    "                dump_i = 0\n",
    "                print(\"4-Dumping\")\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset_test_bls_alt_7.joblib')\n",
    "                joblib.dump([Dataset_global,Dataset_local], 'Dataset1_test_bls_alt_7.joblib')\n",
    "                print(\"4-Dump Complete\")\n",
    "\n",
    "        except :\n",
    "            print(\"4-Download/Stitch error! #\")\n",
    "            index = index + 1 \n",
    "\n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    starttime = time.time()\n",
    "    processes = []\n",
    "\n",
    "    p = multiprocessing.Process(target=one)\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "    \n",
    "    q = multiprocessing.Process(target=two)\n",
    "    processes.append(q)\n",
    "    q.start()\n",
    "    \n",
    "    r = multiprocessing.Process(target=three)\n",
    "    processes.append(r)\n",
    "    r.start()\n",
    "    \n",
    "    s = multiprocessing.Process(target=four)\n",
    "    processes.append(s)\n",
    "    s.start()\n",
    "        \n",
    "    for process in processes:\n",
    "        process.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46268f-9e31-4919-9b20-11269f2b3db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
